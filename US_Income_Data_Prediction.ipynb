{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bnnguyen/USIncomePredictionTraining/blob/main/US_Income_Data_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrZPS-OicHXw"
      },
      "source": [
        "# Data pre-processing\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHpzyLYK9Hrd"
      },
      "source": [
        "## Feature Encoding\n",
        "\n",
        "Some of our features are random strings. So let's first start by encoding them!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MEuH9LlU9GcO"
      },
      "outputs": [],
      "source": [
        "def featureEncoding(df):\n",
        "  import pandas as pd\n",
        "  from sklearn.preprocessing import LabelEncoder\n",
        "  # Create a LabelEncoder instance\n",
        "  le = LabelEncoder()\n",
        "\n",
        "  # Loop through each column (except the last one, which is the target 'income_50k' column)\n",
        "  for column in df.columns[:-1]:\n",
        "    if (df[column].dtypes == 'object'):\n",
        "      df[column] = le.fit_transform(df[column])\n",
        "      column_map = dict(zip(le.classes_, le.transform(le.classes_)))\n",
        "      for d in column_map.items():\n",
        "        if d[0] == '?':\n",
        "          df[column]=df[column].replace(d[1],-1)\n",
        "  # Check if the 'income_50k' column exists before transforming\n",
        "  if 'income_50k' in df.columns:\n",
        "      df['income_50k'] = le.fit_transform(df['income_50k'])\n",
        "      class_map = dict(zip(le.classes_, le.transform(le.classes_)))\n",
        "      print(f\"income_50k encoding: {class_map}\")\n",
        "  else:\n",
        "      print(\"No 'income_50k' column found in the DataFrame.\")\n",
        "  return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C801arEXqY7-"
      },
      "source": [
        "## Feature Imputation\n",
        "\n",
        "Notice that there are some values that are missing. Therefore we will first perform data imputation over the missing data.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgcK5X2b8mEZ"
      },
      "outputs": [],
      "source": [
        "def simpleImputation(df):\n",
        "  from sklearn.impute import SimpleImputer, KNNImputer\n",
        "  # Determine the column names dynamically\n",
        "  column_names = df.columns.tolist()\n",
        "\n",
        "  # Create a list of column names that need imputation (excluding the target column 'income_50k')\n",
        "  columns_to_impute = [col for col in column_names if col != 'income_50k']\n",
        "\n",
        "  # Create an instance of the SimpleImputer and fit-transform the dataset\n",
        "  imputer = SimpleImputer(missing_values=-1,strategy='most_frequent')\n",
        "  df[columns_to_impute] = imputer.fit_transform(df[columns_to_impute])\n",
        "  return df\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6uFNL5kF-2H"
      },
      "source": [
        "You can also use the KNNImputer! Infact, you are encouraged to go through the functions listed here: https://scikit-learn.org/stable/modules/impute.html#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loin8TcPGMtW"
      },
      "source": [
        "## Feature Scaling\n",
        "\n",
        "Now we seet that the values are in different ranges! We can handle this by scaling the values of the DataFrame! Note that this might not always be neccessary, so don't assume that this is something that you have to do for all your problems!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNuxRbMSdinR"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EL7HLEEyGiGC"
      },
      "outputs": [],
      "source": [
        "def featureScaling(df):\n",
        "  from sklearn.preprocessing import MinMaxScaler\n",
        "  from sklearn.impute import SimpleImputer, KNNImputer\n",
        "  scaled_array = MinMaxScaler().fit_transform(df.values)\n",
        "  df = pd.DataFrame(scaled_array, columns = df.columns)\n",
        "  return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLQVE9nZHnfc"
      },
      "source": [
        "## Data Balancing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUrjGugSJU2q"
      },
      "source": [
        "### Using imbalanced-learn\n",
        "\n",
        "imbalanced-learn is a library for working with imbalanced-data. Find more at: https://imbalanced-learn.org/stable/\n",
        "\n",
        "In this example, we will see 3 popuplar methods of working with imbalanced data using the library imbalanced-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_p73u8f6SVn9"
      },
      "source": [
        "Below is an example to use `RandomUnderSampler`. If you specify a number between 0 and 1 for sampling_strategy, it will make sure that the proportion of minority/majority samples is equal to that number."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXf50jHCM0Ae"
      },
      "outputs": [],
      "source": [
        "def Undersampling(df):\n",
        "  from imblearn.under_sampling import RandomUnderSampler\n",
        "  from collections import Counter\n",
        "\n",
        "  column_names = df.columns\n",
        "  # This line splits dataset into features and values\n",
        "  X, y = df[df.columns[:-1]].values.astype(float), df[df.columns[-1]].values.astype(int)\n",
        "  rus = RandomUnderSampler(random_state=0, sampling_strategy='not minority')\n",
        "  X_resampled, y_resampled = rus.fit_resample(X, y)\n",
        "  print(sorted(Counter(y_resampled).items()))\n",
        "  df=pd.DataFrame(X_resampled)\n",
        "  df[\"income_50k\"]=y_resampled\n",
        "  df.columns=column_names\n",
        "  return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_N_URKYuSjHz"
      },
      "source": [
        "Similarly for oversampling..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEuE3rcISHax"
      },
      "outputs": [],
      "source": [
        "def Oversampling(df):\n",
        "  from imblearn.over_sampling import RandomOverSampler\n",
        "  from collections import Counter\n",
        "  column_names = df.columns\n",
        "  # This line splits dataset into features and values\n",
        "  X, y = df[df.columns[:-1]].values.astype(float), df[df.columns[-1]].values.astype(int)\n",
        "  rus = RandomOverSampler(random_state=0, sampling_strategy='not majority')\n",
        "  X_resampled, y_resampled = rus.fit_resample(X, y)\n",
        "  print(sorted(Counter(y_resampled).items()))\n",
        "  df=pd.DataFrame(X_resampled)\n",
        "  df[\"income_50k\"]=y_resampled\n",
        "  df.columns=column_names\n",
        "  return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hbqmf5SvUE3j"
      },
      "source": [
        "Using the SMOTE method is also simple using imbalance-learn:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JSN-0g-sUEZD"
      },
      "outputs": [],
      "source": [
        "def smote(df):\n",
        "  from imblearn.over_sampling import SMOTE\n",
        "  from collections import Counter\n",
        "  column_names = df.columns\n",
        "  # This line splits dataset into features and values\n",
        "  X, y = df[df.columns[:-1]].values.astype(float), df[df.columns[-1]].values.astype(int)\n",
        "  rus = SMOTE(random_state=0, sampling_strategy='not majority', k_neighbors = 5)\n",
        "  X_resampled, y_resampled = rus.fit_resample(X, y)\n",
        "  print(sorted(Counter(y_resampled).items()))\n",
        "  df=pd.DataFrame(X_resampled);\n",
        "  df[\"income_50k\"]=y_resampled\n",
        "  df.columns=column_names\n",
        "  return df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Try to apply PCA"
      ],
      "metadata": {
        "id": "2D-K1DFZ3bKL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "def plot_num_component_vs_explained_variance(X_train):\n",
        "    pca = PCA(n_components=X_train.shape[1])\n",
        "    X_train_pca = pca.fit_transform(X_train)\n",
        "    x_axis = [i+1 for i in range(X_train.shape[1])]\n",
        "    y_axis = [np.sum(pca.explained_variance_ratio_[:i+1]) * 100 for i in range(X_train.shape[1])]\n",
        "\n",
        "    plt.clf()\n",
        "    plt.plot(x_axis, y_axis)\n",
        "    plt.xlabel(\"Number of selected features\")\n",
        "    plt.ylabel(\"%age of explained variance\")\n",
        "    plt.title(\"Num Features vs Explained Variance\")\n",
        "    plt.show()\n",
        "\n",
        "plot_num_component_vs_explained_variance(X_train)"
      ],
      "metadata": {
        "id": "M53oIVym3bKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can almost look at this graph and tell that after about 5 features, there is 0 variance explained.\n",
        "\n",
        "Let's continue with our PCA of 5 features and modified train and test sets."
      ],
      "metadata": {
        "id": "BGZNCn2LgQ_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.decomposition import PCA\n",
        "import time\n",
        "\n",
        "# Load the dataset from Google Drive\n",
        "file_id = '1ArItH0Fuovf5VtsV1p9waK6hMg4uLVTa'\n",
        "url = 'https://drive.google.com/uc?id={}'.format(file_id)\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Apply preprocessing steps\n",
        "df = featureEncoding(df)\n",
        "df = simpleImputation(df)\n",
        "# df = featureScaling(df)\n",
        "df= smote(df)\n",
        "\n",
        "# This line splits dataset into features and values\n",
        "X, y = df[df.columns[:-1]].values.astype(float), df[df.columns[-1]].values.astype(int)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "def make_pca(X_train, num_components):\n",
        "    pca = PCA(n_components = num_components)\n",
        "    X_train_pca = pca.fit_transform(X_train)\n",
        "    return pca, X_train_pca\n",
        "\n",
        "pca_5, X_train_pca_5 = make_pca(X_train, num_components = 5)\n",
        "X_test_pca_5 = pca_5.transform(X_test)"
      ],
      "metadata": {
        "id": "i6pYvFPcyBnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "AQd6pFEj3HZR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## KNN"
      ],
      "metadata": {
        "id": "mK1z3Rsr3T7M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can almost look at this graph and tell that after about 5 features, there is 0 variance explained.\n",
        "\n",
        "Let's continue with our PCA of 5 features and modified train and test sets."
      ],
      "metadata": {
        "id": "tAROTbll3yYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.decomposition import PCA\n",
        "import time\n",
        "\n",
        "# Load the dataset from Google Drive\n",
        "file_id = '1ArItH0Fuovf5VtsV1p9waK6hMg4uLVTa'\n",
        "url = 'https://drive.google.com/uc?id={}'.format(file_id)\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Apply preprocessing steps\n",
        "df = featureEncoding(df)\n",
        "df = simpleImputation(df)\n",
        "# df = featureScaling(df)\n",
        "df= smote(df)\n",
        "\n",
        "# This line splits dataset into features and values\n",
        "X, y = df[df.columns[:-1]].values.astype(float), df[df.columns[-1]].values.astype(int)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "# Plotting fundtion\n",
        "def plot_accuracy(accuracy_per_k, classifier_name):\n",
        "    # Extract the values of k and accuracy from the dictionary\n",
        "    k_values = list(accuracy_per_k.keys())\n",
        "    accuracy = list(accuracy_per_k.values())\n",
        "    # Create a bar chart of the accuracy per k\n",
        "    plt.bar(k_values, accuracy)\n",
        "    plt.xlabel(\"k\")\n",
        "    plt.ylabel(\"Accuracy (%)\")\n",
        "    plt.title(f\"Accuracy of KNN per k for {classifier_name}\")\n",
        "    plt.show()\n",
        "\n",
        "def find_best_k(X_train, X_test, y_train, y_test):\n",
        "    best_k = 0\n",
        "    best_accuracy = 0\n",
        "    accuracy_per_k = {}\n",
        "    for k in range(1, 11):\n",
        "        sklearn_classifier = KNeighborsClassifier(k)\n",
        "        sklearn_classifier.fit(X_train, y_train)\n",
        "        accuracy_per_k[k] = sklearn_classifier.score(X_test, y_test)\n",
        "        if accuracy_per_k[k] > best_accuracy:\n",
        "            best_k = k\n",
        "            best_accuracy = accuracy_per_k[k]\n",
        "    # call the plot_accuracy function\n",
        "    plot_accuracy(accuracy_per_k, \"Classifier\")\n",
        "    return best_k, best_accuracy\n",
        "\n",
        "X_train=X_train_pca_5\n",
        "X_test=X_test_pca_5\n",
        "\n",
        "train_start_time = time.time()\n",
        "best_k, best_score = find_best_k(X_train, X_test, y_train, y_test)\n",
        "train_total_time = time.time() - train_start_time\n",
        "print(f\"The best value of k is {best_k} with accuracy {best_score * 100}%\")\n",
        "print(f\"Time for evaluate the best k: {train_total_time}seconds\")\n",
        "\n",
        "clf = KNeighborsClassifier(best_k)\n",
        "\n",
        "train_start_time = time.time()\n",
        "clf.fit(X_train, y_train)\n",
        "train_total_time = time.time() - train_start_time\n",
        "\n",
        "test_start_time = time.time()\n",
        "score = clf.score(X_test, y_test)\n",
        "test_total_time = time.time() - test_start_time\n",
        "\n",
        "print(f\"Training finished in {train_total_time} seconds\")\n",
        "print(f\"Score: {score*100}%. Scoring took {test_total_time} seconds\")\n",
        "\n",
        "orig_test_start_time = time.time()\n",
        "orig_score = clf.predict([X_test[2]])\n",
        "orig_test_total_time = time.time() - orig_test_start_time\n",
        "print(f\"Predicting the class of 1 sample took {orig_test_total_time} seconds\")"
      ],
      "metadata": {
        "id": "uoq0Y0zmIefP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decision Tree"
      ],
      "metadata": {
        "id": "jNtVKKDg3YUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import tree\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset from Google Drive\n",
        "file_id = '1ArItH0Fuovf5VtsV1p9waK6hMg4uLVTa'\n",
        "url = 'https://drive.google.com/uc?id={}'.format(file_id)\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Apply preprocessing steps\n",
        "df = featureEncoding(df)\n",
        "df = simpleImputation(df)\n",
        "#df = featureScaling(df)\n",
        "df= smote(df)\n",
        "\n",
        "# This line splits dataset into features and values\n",
        "X, y = df[df.columns[:-1]].values.astype(float), df[df.columns[-1]].values.astype(int)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "X_train=X_train_pca_5\n",
        "X_test=X_test_pca_5\n",
        "\n",
        "clf = tree.DecisionTreeClassifier()\n",
        "#clf = tree.DecisionTreeClassifier(criterion = 'gini')\n",
        "\n",
        "orig_train_start_time = time.time()\n",
        "clf.fit(X_train, y_train)\n",
        "orig_train_total_time = time.time() - orig_train_start_time\n",
        "\n",
        "orig_test_start_time = time.time()\n",
        "orig_score = clf.score(X_test, y_test)\n",
        "orig_test_total_time = time.time() - orig_test_start_time\n",
        "\n",
        "print(f\"Training finished in {orig_train_total_time} seconds\")\n",
        "print(f\"Score: {orig_score*100}%. Scoring took {orig_test_total_time} seconds\")\n",
        "\n",
        "orig_test_start_time = time.time()\n",
        "orig_score = clf.predict([X_test[2]])\n",
        "orig_test_total_time = time.time() - orig_test_start_time\n",
        "print(f\"Predicting the class of 1 sample took {orig_test_total_time} seconds\")"
      ],
      "metadata": {
        "id": "BCq2gaztkm65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression"
      ],
      "metadata": {
        "id": "3WxbesXi3iiZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "# Load the dataset from Google Drive\n",
        "file_id = '1ArItH0Fuovf5VtsV1p9waK6hMg4uLVTa'\n",
        "url = 'https://drive.google.com/uc?id={}'.format(file_id)\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Apply preprocessing steps\n",
        "df = featureEncoding(df)\n",
        "df = simpleImputation(df)\n",
        "#df = featureScaling(df)\n",
        "df= smote(df)\n",
        "\n",
        "# This line splits dataset into features and values\n",
        "X, y = df[df.columns[:-1]].values.astype(float), df[df.columns[-1]].values.astype(int)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "X_train=X_train_pca_5\n",
        "X_test=X_test_pca_5\n",
        "# Initialize and train the LogisticRegression model\n",
        "clf = LogisticRegression()\n",
        "\n",
        "orig_train_start_time = time.time()\n",
        "clf.fit(X_train, y_train)\n",
        "orig_train_total_time = time.time() - orig_train_start_time\n",
        "\n",
        "orig_test_start_time = time.time()\n",
        "orig_score = clf.score(X_test, y_test)\n",
        "orig_test_total_time = time.time() - orig_test_start_time\n",
        "\n",
        "print(f\"Training finished in {orig_train_total_time} seconds\")\n",
        "print(f\"Score: {orig_score*100}%. Scoring took {orig_test_total_time} seconds\")\n",
        "\n",
        "orig_test_start_time = time.time()\n",
        "orig_score = clf.predict([X_test[2]])\n",
        "orig_test_total_time = time.time() - orig_test_start_time\n",
        "print(f\"Predicting the class of 1 sample took {orig_test_total_time} seconds\")"
      ],
      "metadata": {
        "id": "lhyPCkFH-cWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Networks"
      ],
      "metadata": {
        "id": "GtWpUcdPaPMM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "import matplotlib.pyplot as plt\n",
        "# Load the dataset from Google Drive\n",
        "file_id = '1ArItH0Fuovf5VtsV1p9waK6hMg4uLVTa'\n",
        "url = 'https://drive.google.com/uc?id={}'.format(file_id)\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Apply preprocessing steps\n",
        "df = featureEncoding(df)\n",
        "df = simpleImputation(df)\n",
        "#df = featureScaling(df)\n",
        "# df= smote(df)\n",
        "\n",
        "# This line splits dataset into features and values\n",
        "X, y = df[df.columns[:-1]].values.astype(float), df[df.columns[-1]].values.astype(int)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n"
      ],
      "metadata": {
        "id": "6xJWbNziaT5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "nn_clf = MLPClassifier(\n",
        "    hidden_layer_sizes = (5,3),\n",
        "    activation = 'relu',\n",
        "    solver = 'adam',\n",
        "    alpha = 0.001,\n",
        "    learning_rate_init = 0.001,\n",
        "    tol = 0.0001,\n",
        "    n_iter_no_change=10,\n",
        "    max_iter = 10000\n",
        ")\n",
        "\n",
        "nn_clf_pca_5 = MLPClassifier(\n",
        "    hidden_layer_sizes = (5,3),\n",
        "    activation = 'relu',\n",
        "    solver = 'adam',\n",
        "    alpha = 0.001,\n",
        "    learning_rate_init = 0.001,\n",
        "    tol = 0.0001,\n",
        "    n_iter_no_change=10,\n",
        "    max_iter = 10000\n",
        ")\n",
        "pca_5, X_train_pca_5 = make_pca(X_train, num_components = 5)\n",
        "X_test_pca_5 = pca_5.transform(X_test)\n",
        "\n",
        "import time\n",
        "\n",
        "pca_5_train_start_time = time.time()\n",
        "nn_clf_pca_5.fit(X_train_pca_5, y_train)\n",
        "pca_5_train_total_time = time.time() - pca_5_train_start_time\n",
        "\n",
        "pca_5_test_start_time = time.time()\n",
        "pca_5_score = nn_clf_pca_5.score(X_test_pca_5, y_test)\n",
        "pca_5_test_total_time = time.time() - pca_5_test_start_time\n",
        "\n",
        "print(\"With reduced dataframe:\")\n",
        "print(f\"Training finished in {pca_5_train_total_time} seconds\")\n",
        "print(f\"Score: {pca_5_score*100}%. Scoring took {pca_5_test_total_time} seconds\")\n",
        "\n",
        "orig_test_start_time = time.time()\n",
        "orig_score = nn_clf_pca_5.predict([X_test_pca_5[2]])\n",
        "orig_test_total_time = time.time() - orig_test_start_time\n",
        "print(f\"Predicting the class of 1 sample took {orig_test_total_time} seconds\")\n",
        "\n",
        "orig_train_start_time = time.time()\n",
        "nn_clf.fit(X_train, y_train)\n",
        "orig_train_total_time = time.time() - orig_train_start_time\n",
        "\n",
        "orig_test_start_time = time.time()\n",
        "orig_score = nn_clf.score(X_test, y_test)\n",
        "orig_test_total_time = time.time() - orig_test_start_time\n",
        "\n",
        "print(\"With original dataframe:\")\n",
        "print(f\"Training finished in {orig_train_total_time} seconds\")\n",
        "print(f\"Score: {orig_score*100}%. Scoring took {orig_test_total_time} seconds\")\n",
        "\n",
        "orig_test_start_time = time.time()\n",
        "orig_score = nn_clf.predict([X[2]])\n",
        "orig_test_total_time = time.time() - orig_test_start_time\n",
        "print(f\"Predicting the class of 1 sample took {orig_test_total_time} seconds\")"
      ],
      "metadata": {
        "id": "pHU0JwtIgvWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing program"
      ],
      "metadata": {
        "id": "hQ16c9vZ2_FJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wpj2Pp_hcGFF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "from collections import Counter\n",
        "\n",
        "# Replace 'your_file.csv' with the actual path to your CSV file\n",
        "file_id = '1ArItH0Fuovf5VtsV1p9waK6hMg4uLVTa'\n",
        "url = 'https://drive.google.com/uc?id={}'.format(file_id)\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Preprocessing\n",
        "df=featureEncoding(df)\n",
        "# This line splits dataset into features and values\n",
        "X, y = df[df.columns[:-1]].values.astype(float), df[df.columns[-1]].values.astype(int)\n",
        "print(sorted(Counter(y).items()))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}